{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(3407)\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 1, 2, 3, 11, 13, 10, 4, 5, 6, 11, 12, 10, 6, 11, 14, 17]\n",
      "123+456*6=\n"
     ]
    }
   ],
   "source": [
    "StT = {\n",
    "    \"0\": 0, \"1\": 1, \"2\": 2, \"3\": 3, \"4\": 4, \"5\": 5, \"6\": 6, \"7\": 7, \"8\": 8, \"9\": 9,\n",
    "    \"NumBeg\": 10, \"NumEnd\": 11, \"*\": 12, \"+\": 13, \"=\": 14, \"ThinkBeg\": 15, \"ThinkEnd\": 16, \"Eos\": 17\n",
    "}\n",
    "\n",
    "TtS = {v: k for k, v in StT.items()}\n",
    "\n",
    "def tokenize(s):\n",
    "    out = []\n",
    "    num = False\n",
    "    past = \"\"\n",
    "    for c in s:\n",
    "        chars = past + c\n",
    "        if chars in StT:\n",
    "            tok = StT[chars]\n",
    "            if tok < 10:\n",
    "                if not num:\n",
    "                    out.append(StT[\"NumBeg\"])\n",
    "                    num = True\n",
    "                out.append(tok)\n",
    "            else:\n",
    "                if num:\n",
    "                    out.append(StT[\"NumEnd\"])\n",
    "                    num = False\n",
    "                out.append(tok)\n",
    "            past = \"\"\n",
    "        else:\n",
    "            past += c\n",
    "\n",
    "    return out + [StT[\"Eos\"]]\n",
    "\n",
    "def detokenize(toks):\n",
    "    out = []\n",
    "    num = False\n",
    "    for tok in toks:\n",
    "        if tok == 10 or tok == 11 or (tok >= 15 and tok <= 17) :\n",
    "            continue\n",
    "        out.append(TtS[tok])\n",
    "\n",
    "    return \"\".join(out)\n",
    "\n",
    "example = \"123+456*6=\"\n",
    "print(tokenize(example))\n",
    "print(detokenize(tokenize(example)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644*314=ThinkBeg600*314+40*314+4*314=188400+12560+1256=ThinkEnd202216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step_mul(length):\n",
    "    a = [random.randint(0,9) for i in range(length)]\n",
    "    b = [random.randint(0,9) for i in range(length)]\n",
    "    # a = [9, 9, 9]\n",
    "    # b = [9, 9, 9]\n",
    "\n",
    "    val_a = int(''.join(str(d) for d in a))\n",
    "    val_b = int(''.join(str(d) for d in b))\n",
    "    if val_a < val_b:\n",
    "        val_a, val_b = val_b, val_a\n",
    "        a, b = b, a\n",
    "    \n",
    "    string = f\"{val_a}*{val_b}=ThinkBeg\"\n",
    "\n",
    "    steps_eq = []\n",
    "    for i in range(length):\n",
    "        a_i = a[i] * 10**(length-i-1)\n",
    "        # a_i = str(a[i]) \n",
    "        string += f\"{a_i}*{val_b}+\"\n",
    "        steps_eq.append(str(a_i*val_b)+\"+\")\n",
    "\n",
    "    string = string[:-1] + \"=\" + \"\".join(steps_eq)[:-1] + \"=\" + \"ThinkEnd\" + str(val_a*val_b)\n",
    "\n",
    "    return string\n",
    "    # print(out)\n",
    "\n",
    "eq = step_mul(3)\n",
    "print(eq)\n",
    "tokenize(eq)\n",
    "len(tokenize(eq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulDataset(Dataset):\n",
    "    \"\"\" \n",
    "    Dataset for the Add problem. E.g. for problem length 3:\n",
    "    12 + 333 = 345\n",
    "    Input: 0 1 2 3 3 3 -> Output: 0 3 4 5\n",
    "    Which will feed into the transformer concatenated as:\n",
    "    input:  0 1 2 3 3 3 0 3 4\n",
    "    output: I I I I I 0 3 4 5\n",
    "    where I is \"ignore\", as the transformer is reading the input sequence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split, length=3):\n",
    "        assert split in {'train', 'test'}\n",
    "        self.split = split\n",
    "        self.length = length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 100000 # ...\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(StT)\n",
    "    \n",
    "    def get_block_size(self):\n",
    "        # the length of the sequence that will feed into transformer, \n",
    "        # containing concatenated input and the output, but -1 because\n",
    "        # the transformer starts making predictions at the last input element\n",
    "        return 79\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        while True:\n",
    "            rai = tokenize(step_mul(self.length))\n",
    "            h = hash(str(rai[:1+2*(self.length+2)]))\n",
    "            \n",
    "            inp_split = 'test' if h % 4 == 0 else 'train' # designate 25% of examples as test\n",
    "            if inp_split == self.split:\n",
    "                break # ok\n",
    "        \n",
    "        if len(rai) < self.get_block_size():\n",
    "            rai += [StT[\"Eos\"]] * (self.get_block_size() - len(rai))\n",
    "        \n",
    "        x = torch.tensor(rai[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(rai[1:], dtype=torch.long)\n",
    "        \n",
    "        # we only want to predict at output locations, mask out the loss at the input locations\n",
    "        y[:2*(self.length+2)+1] = -1\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10,  3,  5,  9, 11, 12, 10,  1,  1,  9, 11, 14, 15, 10,  3,  0,  0, 11,\n",
      "        12, 10,  1,  1,  9, 11, 13, 10,  5,  0, 11, 12, 10,  1,  1,  9, 11, 13,\n",
      "        10,  9, 11, 12, 10,  1,  1,  9, 11, 14, 10,  3,  5,  7,  0,  0, 11, 13,\n",
      "        10,  5,  9,  5,  0, 11, 13, 10,  1,  0,  7,  1, 11, 14, 16, 10,  4,  2,\n",
      "         7,  2,  1, 17, 17, 17])\n",
      "10 -1\n",
      "3 -1\n",
      "5 -1\n",
      "9 -1\n",
      "11 -1\n",
      "12 -1\n",
      "10 -1\n",
      "1 -1\n",
      "1 -1\n",
      "9 -1\n",
      "11 -1\n",
      "14 15\n",
      "15 10\n",
      "10 3\n",
      "3 0\n",
      "0 0\n",
      "0 11\n",
      "11 12\n",
      "12 10\n",
      "10 1\n",
      "1 1\n",
      "1 9\n",
      "9 11\n",
      "11 13\n",
      "13 10\n",
      "10 5\n",
      "5 0\n",
      "0 11\n",
      "11 12\n",
      "12 10\n",
      "10 1\n",
      "1 1\n",
      "1 9\n",
      "9 11\n",
      "11 13\n",
      "13 10\n",
      "10 9\n",
      "9 11\n",
      "11 12\n",
      "12 10\n",
      "10 1\n",
      "1 1\n",
      "1 9\n",
      "9 11\n",
      "11 14\n",
      "14 10\n",
      "10 3\n",
      "3 5\n",
      "5 7\n",
      "7 0\n",
      "0 0\n",
      "0 11\n",
      "11 13\n",
      "13 10\n",
      "10 5\n",
      "5 9\n",
      "9 5\n",
      "5 0\n",
      "0 11\n",
      "11 13\n",
      "13 10\n",
      "10 1\n",
      "1 0\n",
      "0 7\n",
      "7 1\n",
      "1 11\n",
      "11 14\n",
      "14 16\n",
      "16 10\n",
      "10 4\n",
      "4 2\n",
      "2 7\n",
      "7 2\n",
      "2 1\n",
      "1 17\n",
      "17 17\n",
      "17 17\n",
      "17 17\n"
     ]
    }
   ],
   "source": [
    "# print an example instance of the dataset\n",
    "train_dataset = MulDataset('train')\n",
    "test_dataset = MulDataset('test')\n",
    "x, y = train_dataset[0]\n",
    "\n",
    "print (x)\n",
    "for a, b in zip(x,y):\n",
    "    print(int(a),int(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 2.69M\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT\n",
    "\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt-mini'\n",
    "# model_config.model_type = 'gpt-nano'\n",
    "\n",
    "model_config.vocab_size = train_dataset.get_vocab_size()\n",
    "model_config.block_size = train_dataset.get_block_size()\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 6 192\n"
     ]
    }
   ],
   "source": [
    "print (model_config.n_head, model_config.n_layer, model_config.n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cuda\n"
     ]
    }
   ],
   "source": [
    "# create a Trainer object\n",
    "from mingpt.trainer import Trainer\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 1e-4 # the model we're using is so small that we can go a bit faster\n",
    "train_config.max_iters = 5000\n",
    "train_config.num_workers = 0\n",
    "# train_config.batch_size = 32\n",
    "trainer = Trainer(train_config, model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_dt 0.00ms; iter 0: train loss 2.92101\n",
      "iter_dt 153.22ms; iter 100: train loss 0.99722\n",
      "iter_dt 150.28ms; iter 200: train loss 0.66158\n",
      "iter_dt 151.06ms; iter 300: train loss 0.53818\n",
      "iter_dt 150.92ms; iter 400: train loss 0.49831\n",
      "iter_dt 150.03ms; iter 500: train loss 0.47740\n",
      "iter_dt 150.55ms; iter 600: train loss 0.42569\n",
      "iter_dt 151.64ms; iter 700: train loss 0.39971\n",
      "iter_dt 150.93ms; iter 800: train loss 0.37731\n",
      "iter_dt 150.43ms; iter 900: train loss 0.33280\n",
      "iter_dt 151.50ms; iter 1000: train loss 0.30704\n",
      "iter_dt 150.90ms; iter 1100: train loss 0.29579\n",
      "iter_dt 162.32ms; iter 1200: train loss 0.27111\n",
      "iter_dt 151.02ms; iter 1300: train loss 0.26446\n",
      "iter_dt 152.03ms; iter 1400: train loss 0.24715\n",
      "iter_dt 150.52ms; iter 1500: train loss 0.24588\n",
      "iter_dt 151.00ms; iter 1600: train loss 0.23221\n",
      "iter_dt 154.69ms; iter 1700: train loss 0.20418\n",
      "iter_dt 151.75ms; iter 1800: train loss 0.19791\n",
      "iter_dt 151.38ms; iter 1900: train loss 0.18220\n",
      "iter_dt 150.62ms; iter 2000: train loss 0.15697\n",
      "iter_dt 152.32ms; iter 2100: train loss 0.16391\n",
      "iter_dt 151.85ms; iter 2200: train loss 0.16174\n",
      "iter_dt 151.97ms; iter 2300: train loss 0.14881\n",
      "iter_dt 151.53ms; iter 2400: train loss 0.13475\n",
      "iter_dt 152.44ms; iter 2500: train loss 0.14704\n",
      "iter_dt 153.41ms; iter 2600: train loss 0.12939\n",
      "iter_dt 152.13ms; iter 2700: train loss 0.13215\n",
      "iter_dt 151.29ms; iter 2800: train loss 0.13042\n",
      "iter_dt 151.60ms; iter 2900: train loss 0.11204\n",
      "iter_dt 150.65ms; iter 3000: train loss 0.11122\n",
      "iter_dt 151.93ms; iter 3100: train loss 0.11534\n",
      "iter_dt 151.09ms; iter 3200: train loss 0.10409\n",
      "iter_dt 150.25ms; iter 3300: train loss 0.09469\n",
      "iter_dt 151.38ms; iter 3400: train loss 0.09792\n",
      "iter_dt 144.62ms; iter 3500: train loss 0.10484\n",
      "iter_dt 150.93ms; iter 3600: train loss 0.08919\n",
      "iter_dt 150.63ms; iter 3700: train loss 0.08847\n",
      "iter_dt 166.25ms; iter 3800: train loss 0.09287\n",
      "iter_dt 148.49ms; iter 3900: train loss 0.08756\n",
      "iter_dt 151.68ms; iter 4000: train loss 0.07823\n",
      "iter_dt 167.13ms; iter 4100: train loss 0.07622\n",
      "iter_dt 158.25ms; iter 4200: train loss 0.07364\n",
      "iter_dt 159.58ms; iter 4300: train loss 0.06978\n",
      "iter_dt 156.98ms; iter 4400: train loss 0.06672\n",
      "iter_dt 151.01ms; iter 4500: train loss 0.07292\n",
      "iter_dt 151.44ms; iter 4600: train loss 0.06577\n",
      "iter_dt 151.30ms; iter 4700: train loss 0.06566\n",
      "iter_dt 151.21ms; iter 4800: train loss 0.05910\n",
      "iter_dt 151.15ms; iter 4900: train loss 0.06136\n"
     ]
    }
   ],
   "source": [
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's perform some evaluation\n",
    "model.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10,  9,  8,  4, 11, 12, 10,  2,  1,  8, 11, 14])\n",
      "tensor([10,  2,  1,  4,  5,  1,  2, 17])\n",
      "10 10\n",
      "9 2\n",
      "8 1\n",
      "4 4\n",
      "11 5\n",
      "12 1\n",
      "10 2\n",
      "2 17\n"
     ]
    }
   ],
   "source": [
    "class EvalMulDataset(Dataset):\n",
    "    \"\"\" \n",
    "    Dataset for the Add problem. E.g. for problem length 3:\n",
    "    12 + 333 = 345\n",
    "    Input: 0 1 2 3 3 3 -> Output: 0 3 4 5\n",
    "    Which will feed into the transformer concatenated as:\n",
    "    input:  0 1 2 3 3 3 0 3 4\n",
    "    output: I I I I I 0 3 4 5\n",
    "    where I is \"ignore\", as the transformer is reading the input sequence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split, length=3):\n",
    "        assert split in {'train', 'test'}\n",
    "        self.split = split\n",
    "        self.length = length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 100000 # ...\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(StT)\n",
    "    \n",
    "    def get_block_size(self):\n",
    "        # the length of the sequence that will feed into transformer, \n",
    "        # containing concatenated input and the output, but -1 because\n",
    "        # the transformer starts making predictions at the last input element\n",
    "        return 79\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        while True:\n",
    "            rai = tokenize(step_mul(self.length))\n",
    "            h = hash(str(rai[:1+2*(self.length+2)]))\n",
    "            \n",
    "            inp_split = 'test' if h % 4 == 0 else 'train' # designate 25% of examples as test\n",
    "            if inp_split == self.split:\n",
    "                break # ok\n",
    "        \n",
    "        x = torch.tensor(rai[:-1], dtype=torch.long)[:rai.index(StT[\"=\"])+1]\n",
    "        y = torch.tensor(rai[1:], dtype=torch.long)[rai.index(StT[\"ThinkEnd\"]):]\n",
    "        \n",
    "        # we only want to predict at output locations, mask out the loss at the input locations\n",
    "        return x, y\n",
    "    \n",
    "eval_train_dataset = EvalMulDataset('train')\n",
    "eval_test_dataset = EvalMulDataset('test')\n",
    "x, y = eval_train_dataset[0]\n",
    "\n",
    "print (x)\n",
    "print (y)\n",
    "for a, b in zip(x,y):\n",
    "    print(int(a),int(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10,  8,  3,  8, 11, 12, 10,  9,  5, 11, 14, 17, 17, 17, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 17, 17, 17, 17, 17, 17,  0,  0,  7,  3,  2,  0,  0,  0,\n",
      "          0,  0,  2,  3,  8,  0,  0, 11, 13, 10,  7,  2,  0,  0, 11, 13, 10,  9,\n",
      "          2,  2,  1,  0, 11, 13, 10,  7,  2, 11, 12, 10,  5,  2,  0, 11, 13, 10,\n",
      "          5, 11, 12, 10,  0, 11, 12, 10,  0, 11, 12, 10,  0, 11, 14, 16, 10,  0,\n",
      "         11, 14, 16, 10,  0, 11, 14, 16, 10,  0, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 17, 17]], device='cuda:0')\n",
      "838*95=\n",
      "838*95=007320000023800+7200+92210+72*520+5*0*0*0=0=0=0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def evaluate_model_accuracy(model, eval_dataset):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(eval_dataset)):\n",
    "            x, y = eval_dataset[i]\n",
    "            x = x.unsqueeze(0).to(device)\n",
    "            y = y.unsqueeze(0).to(device)\n",
    "\n",
    "            output = model.generate(x, 79, do_sample=False)\n",
    "            # print(detokenize(x[0].cpu().numpy()))\n",
    "            # print(detokenize(y[0].cpu().numpy()))\n",
    "            y = int(detokenize(y[0].cpu().numpy()))\n",
    "            predicted = int(detokenize(output[0].cpu().numpy()).split(\"=\")[-1])\n",
    "            # print(predicted)\n",
    "\n",
    "            # Compare predicted and actual values\n",
    "            correct += 1 if y == predicted else 0\n",
    "            total += 1\n",
    "\n",
    "            if total % 100 == 0:\n",
    "                print(f\"Accuracy: {correct}/{total} = {correct/total:.2f}\")\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "input_tensor = torch.tensor(tokenize(\"838*95=\"), dtype=torch.long).unsqueeze(0).to(device)\n",
    "out = model.generate(input_tensor, 120, do_sample=False)\n",
    "print(out)\n",
    "print(detokenize(input_tensor[0].cpu().numpy()))\n",
    "print(detokenize(out[0].cpu().numpy()))\n",
    "# accuracy = evaluate_model_accuracy(model, eval_test_dataset)\n",
    "# print(f\"Model accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_step_85.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
